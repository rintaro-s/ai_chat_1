from huggingface_hub import login
import streamlit as st
import torch
import gc
from PIL import Image
import os
import time
import uuid
from tempfile import NamedTemporaryFile
from transformers import (
    AutoProcessor, AutoTokenizer, AutoModelForCausalLM, AutoModel,
    LlavaForConditionalGeneration, BitsAndBytesConfig, TextIteratorStreamer
)
import warnings

# deprecationè­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore", category=FutureWarning)

# èªè¨¼æƒ…å ± - æœ¬ç•ªç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹ãªã©ã‚»ã‚­ãƒ¥ã‚¢ã«ç®¡ç†
HF_TOKEN = ""
login(token=HF_TOKEN)

# Streamlitè¨­å®š
st.set_page_config(page_title="é«˜æ€§èƒ½AIç”»åƒç†è§£ãƒãƒ£ãƒƒãƒˆ", layout="wide", initial_sidebar_state="expanded")

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å®šç¾© - å®‰å®šã—ã¦å‹•ä½œã™ã‚‹ä¿¡é ¼æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’å«ã‚ã‚‹
VISION_MODELS = {
    "llava": {
        "name": "LLaVA-1.5 (7B)",
        "path": "llava-hf/llava-1.5-7b-hf",
        "vram": 7.0,
        "type": "vision_language",
        "loader_class": LlavaForConditionalGeneration,
        "processor_class": AutoProcessor,
        "description": "è‹±èªã«å¼·ã„è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«",
        "prompt_template": "USER: <image>\n{prompt}\nASSISTANT:"
    },
    "japanese-llava": {
        "name": "Japanese LLaVA (7B)",
        "path": "llava-jp/llava-jp-13b-instruct-lora-jaster-v1.0-to-llava1.6",
        "vram": 7.0,
        "type": "vision_language",
        "loader_class": LlavaForConditionalGeneration,
        "processor_class": AutoProcessor,
        "description": "æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«",
        "prompt_template": "USER: <image>\n{prompt}\nASSISTANT:"
    }
}

TEXT_MODELS = {
    "standard": {
        "elyza": {
            "name": "ELYZA-japanese-Llama-3-8b",
            "path": "elyza/Llama-3-ELYZA-JP-8B",
            "vram": 8.0,
            "loader_class": AutoModelForCausalLM,
            "processor_class": AutoTokenizer,
            "description": "æ—¥æœ¬èªã«æœ€é©åŒ–ã•ã‚ŒãŸLlama3ãƒ¢ãƒ‡ãƒ«"
        },
        "cyberagent": {
            "name": "Calm2-7B",
            "path": "cyberagent/calm2-7b-chat",
            "vram": 7.0,
            "loader_class": AutoModelForCausalLM,
            "processor_class": AutoTokenizer,
            "description": "è‡ªç„¶ãªæ—¥æœ¬èªå¯¾è©±ã«å¼·ã„ãƒ¢ãƒ‡ãƒ«"
        }
    },
    "analytic": {
        "microsoft-phi3": {
            "name": "Phi-3 Mini (7B)",
            "path": "microsoft/phi-3-mini-4k-instruct",
            "vram": 7.0,
            "loader_class": AutoModelForCausalLM,
            "processor_class": AutoTokenizer,
            "description": "æ¨è«–ãƒ»åˆ†æã«ç‰¹åŒ–ã—ãŸã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãƒ¢ãƒ‡ãƒ«"
        },
        "qwen": {
            "name": "Qwen2-7B-Instruct",
            "path": "Qwen/Qwen2-7B-Instruct",
            "vram": 7.0,
            "loader_class": AutoModelForCausalLM,
            "processor_class": AutoTokenizer,
            "description": "è©³ç´°ãªåˆ†æã«å‘ã„ãŸå¤šè¨€èªãƒ¢ãƒ‡ãƒ«"
        }
    },
    "creative": {
        "stable-lm": {
            "name": "Japanese-StableLM",
            "path": "stabilityai/japanese-stablelm-instruct-gamma-7b",
            "vram": 7.0,
            "loader_class": AutoModelForCausalLM,
            "processor_class": AutoTokenizer,
            "description": "å‰µé€ çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å„ªã‚ŒãŸæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«"
        }
    }
}

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "vision_model" not in st.session_state:
    st.session_state.vision_model = None
if "vision_processor" not in st.session_state:
    st.session_state.vision_processor = None
if "text_model" not in st.session_state:
    st.session_state.text_model = None
if "text_processor" not in st.session_state:
    st.session_state.text_processor = None
if "current_mode" not in st.session_state:
    st.session_state.current_mode = "standard"
if "current_vision_model" not in st.session_state:
    st.session_state.current_vision_model = None
if "current_text_model" not in st.session_state:
    st.session_state.current_text_model = None
if "temp_files" not in st.session_state:
    st.session_state.temp_files = []

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†
def cleanup_temp_files():
    for file_path in st.session_state.temp_files:
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception as e:
                pass
    st.session_state.temp_files = []

# ã‚¢ãƒ—ãƒªçµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
if st.session_state.get("cleanup_required", False):
    cleanup_temp_files()
st.session_state["cleanup_required"] = True

# CUDAæƒ…å ±è¡¨ç¤º
def display_cuda_info():
    if torch.cuda.is_available():
        st.sidebar.success(f"CUDA: {torch.cuda.get_device_name(0)}")
        st.sidebar.success(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        st.sidebar.error("CUDAåˆ©ç”¨ä¸å¯")

# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã¨ä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def display_memory_status():
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
        allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
        free_memory = total_memory - reserved_memory
        
        memory_info = (
            f"ç·VRAM: {total_memory:.2f}GB | "
            f"ä½¿ç”¨ä¸­: {reserved_memory:.2f}GB | "
            f"ç©ºã: {free_memory:.2f}GB"
        )
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã«ã‚ˆã£ã¦è‰²ã‚’å¤‰ãˆã‚‹
        memory_percent = (reserved_memory / total_memory) * 100
        if memory_percent > 90:
            st.error(memory_info)
        elif memory_percent > 75:
            st.warning(memory_info)
        else:
            st.success(memory_info)
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º
        st.progress(reserved_memory / total_memory)
        
        # ç¾åœ¨ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
        loaded_models = []
        if st.session_state.vision_model is not None:
            loaded_models.append(f"ç”»åƒ: {st.session_state.current_vision_model}")
        if st.session_state.text_model is not None:
            loaded_models.append(f"ãƒ†ã‚­ã‚¹ãƒˆ: {st.session_state.current_text_model}")
        
        if loaded_models:
            st.info("ğŸ“Œ ç¾åœ¨ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: " + " | ".join(loaded_models))
        else:
            st.info("ãƒ¢ãƒ‡ãƒ«ã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")

# ãƒ¢ãƒ‡ãƒ«ç®¡ç†é–¢æ•°ã®æœ€é©åŒ–
def unload_models(force=False):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
    force=True ã®å ´åˆã¯å¼·åˆ¶çš„ã«ã™ã¹ã¦ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
    force=False ã®å ´åˆã¯å¿…è¦ã«å¿œã˜ã¦ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ã¯1å›ã ã‘ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    if not hasattr(st.session_state, "model_loading") or not st.session_state.model_loading:
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
        if torch.cuda.is_available():
            reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
            total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            memory_usage_percent = reserved_memory / total_memory * 100
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ80%ã‚’è¶…ãˆã‚‹å ´åˆã¾ãŸã¯force=Trueã®å ´åˆã®ã¿ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
            if force or memory_usage_percent > 80:
                if st.session_state.vision_model is not None:
                    del st.session_state.vision_model
                    st.session_state.vision_model = None
                if st.session_state.text_model is not None:
                    del st.session_state.text_model
                    st.session_state.text_model = None
                gc.collect()
                torch.cuda.empty_cache()
                return True
        return False
    return False

def load_vision_model(model_key):
    """ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•° - æ—¢ã«åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—"""
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒãƒˆ
    st.session_state.model_loading = True
    
    # åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    if (st.session_state.vision_model is not None and 
        st.session_state.current_vision_model == VISION_MODELS[model_key]["name"]):
        st.session_state.model_loading = False
        return st.session_state.vision_model, st.session_state.vision_processor, st.session_state.current_vision_model
    
    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«ã€å¿…è¦ã«å¿œã˜ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿
    if st.session_state.vision_model is not None:
        # ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã€ãã®ãƒ¢ãƒ‡ãƒ«ã®ã¿è§£æ”¾
        del st.session_state.vision_model
        st.session_state.vision_model = None
        gc.collect()
        torch.cuda.empty_cache()
    
    try:
        model_info = VISION_MODELS[model_key]
        
        # é‡å­åŒ–è¨­å®š
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        with st.spinner(f"{model_info['name']} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            if torch.cuda.is_available():
                reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                available_memory = total_memory - reserved_memory
                
                if available_memory < model_info["vram"] + 1.0:  # 1GBã®ãƒãƒƒãƒ•ã‚¡ãƒ¼ã‚’è¿½åŠ 
                    st.warning("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾ã—ã¾ã™...")
                    unload_models(force=True)  # ãƒ¡ãƒ¢ãƒªä¸è¶³ãªã‚‰å¼·åˆ¶è§£æ”¾
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼/ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ­ãƒ¼ãƒ‰
            processor = model_info["processor_class"].from_pretrained(
                model_info["path"],
                trust_remote_code=True,
                token=HF_TOKEN
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            model = model_info["loader_class"].from_pretrained(
                model_info["path"],
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                quantization_config=quantization_config,
                token=HF_TOKEN
            )
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã€ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.model_loading = False
            return model, processor, model_info["name"]
    except Exception as e:
        st.session_state.model_loading = False
        st.error(f"ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None

def load_text_model(model_key, mode="standard"):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•° - æ—¢ã«åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—"""
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒãƒˆ
    st.session_state.model_loading = True
    
    model_info = TEXT_MODELS[mode][model_key]
    
    # åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    if (st.session_state.text_model is not None and 
        st.session_state.current_text_model == model_info["name"]):
        st.session_state.model_loading = False
        return st.session_state.text_model, st.session_state.text_processor, st.session_state.current_text_model
    
    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«ã€å¿…è¦ã«å¿œã˜ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿
    if st.session_state.text_model is not None:
        # ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã€ãã®ãƒ¢ãƒ‡ãƒ«ã®ã¿è§£æ”¾
        del st.session_state.text_model
        st.session_state.text_model = None
        gc.collect()
        torch.cuda.empty_cache()
    
    try:
        # é‡å­åŒ–è¨­å®š - ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã«é‡å­åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        with st.spinner(f"{model_info['name']} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            if torch.cuda.is_available():
                reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                available_memory = total_memory - reserved_memory
                
                if available_memory < model_info["vram"] + 1.0:  # 1GBã®ãƒãƒƒãƒ•ã‚¡ãƒ¼ã‚’è¿½åŠ 
                    st.warning("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾ã—ã¾ã™...")
                    unload_models(force=True)  # ãƒ¡ãƒ¢ãƒªä¸è¶³ãªã‚‰å¼·åˆ¶è§£æ”¾
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼/ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ­ãƒ¼ãƒ‰
            processor = model_info["processor_class"].from_pretrained(
                model_info["path"],
                trust_remote_code=True,
                token=HF_TOKEN
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            model = model_info["loader_class"].from_pretrained(
                model_info["path"],
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                quantization_config=quantization_config,
                token=HF_TOKEN
            )
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã€ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.model_loading = False
            return model, processor, model_info["name"]
    except Exception as e:
        st.session_state.model_loading = False
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None

# ç”»åƒå‡¦ç†é–¢æ•°
def optimize_image(image, max_size=1024):
    """ç”»åƒã‚’æœ€é©åŒ–"""
    width, height = image.size
    if width > max_size or height > max_size:
        if width > height:
            new_width = max_size
            new_height = int(height * (max_size / width))
        else:
            new_height = max_size
            new_width = int(width * (max_size / height))
        image = image.resize((new_width, new_height), Image.LANCZOS)
    
    # RGBå½¢å¼ã«å¤‰æ›
    if image.mode == 'RGBA':
        background = Image.new('RGB', image.size, (255, 255, 255))
        background.paste(image, mask=image.split()[3])
        image = background
    elif image.mode != 'RGB':
        image = image.convert('RGB')
    
    return image

def analyze_image(model, processor, image_path, mode="standard"):
    """ç”»åƒåˆ†æã‚’è¡Œã†é–¢æ•°"""
    try:
        image = Image.open(image_path)
        
        # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if mode == "standard":
            prompt = "ã“ã®ç”»åƒã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        elif mode == "analytic":
            prompt = "ã“ã®ç”»åƒã‚’è©³ç´°ã«åˆ†æã—ã€å«ã¾ã‚Œã‚‹æƒ…å ±ã€ç‰©ä½“ã€äººç‰©ã€æ–‡å­—ã€æ•°å­—ãªã©ã‚’è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        else:  # creative
            prompt = "ã“ã®ç”»åƒã‚’è¦‹ã¦ã€é€£æƒ³ã•ã‚Œã‚‹ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚„æ„Ÿæƒ…ã‚’è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚"
        
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
        model_class = model.__class__.__name__.lower()
        
        # LLaVAç³»ã®ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹æ®Šãªå‡¦ç†ãŒå¿…è¦
        if "llava" in model_class:
            try:
                # LLaVAå½¢å¼ã§ã®å‡¦ç†
                messages = [{"role": "user", "content": [{"type": "text", "text": prompt}, {"type": "image_url", "image_url": {"url": f"file://{image_path}"}}]}]
                
                # LLaVAå½¢å¼ã§ã®å…¥åŠ›æº–å‚™
                input_ids = processor.apply_chat_template(
                    messages, 
                    return_tensors="pt"
                ).to(model.device)
                
                # ç”Ÿæˆ
                with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):
                    output = model.generate(
                        input_ids=input_ids,
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.5,
                        top_p=0.95,
                    )
                
                # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                response = processor.batch_decode(output[:, input_ids.shape[1]:], skip_special_tokens=True)[0]
                return response
                
            except Exception as llava_e:
                st.warning(f"LLaVAå½¢å¼ã§ã®å‡¦ç†ã«å¤±æ•—: {llava_e}")
                # ä»£æ›¿å‡¦ç†æ–¹æ³•ã‚’è©¦ã™
                try:
                    # ç¬¬2ã®æ–¹æ³•: å˜ç´”ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨
                    vison_model_info = next((v for k, v in VISION_MODELS.items() if model_class in k.lower()), None)
                    template = vison_model_info['prompt_template'] if vison_model_info else "USER: <image>\n{prompt}\nASSISTANT:"
                    formatted_prompt = template.format(prompt=prompt)
                    
                    # ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆ
                    inputs = processor(text=formatted_prompt, images=image, return_tensors="pt").to(model.device)
                    
                    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):
                        output = model.generate(
                            **inputs,
                            max_new_tokens=256,
                            do_sample=True,
                            temperature=0.5
                        )
                    
                    response = processor.batch_decode(output, skip_special_tokens=True)[0]
                    if prompt in response:
                        response = response.replace(prompt, "").strip()
                    
                    return response
                    
                except Exception as e2:
                    st.warning(f"ä»£æ›¿æ–¹æ³•ã§ã‚‚å¤±æ•—: {e2}")
                    # æœ€çµ‚çš„ãªä»£æ›¿æ–¹æ³•
                    conversation = [{"role": "user", "content": f"{prompt}"}]
                    input_ids = processor.apply_chat_template(conversation, return_tensors="pt").to(model.device)
                    
                    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):
                        output = model.generate(input_ids, max_new_tokens=256)
                    
                    response = processor.batch_decode(output[:, input_ids.shape[1]:], skip_special_tokens=True)[0]
                    return f"ç”»åƒåˆ†æçµæœ: {response}"
        else:
            # ä¸€èˆ¬çš„ãªVLMãƒ¢ãƒ‡ãƒ«ã®å‡¦ç†
            with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):
                try:
                    # æ¨™æº–ã®æ–¹æ³•ã‚’è©¦ã™
                    inputs = processor(text=prompt, images=image, return_tensors="pt").to(model.device)
                    
                    output = model.generate(
                        **inputs,
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.5,
                        use_cache=True
                    )
                    
                    response = processor.batch_decode(output, skip_special_tokens=True)[0]
                    if prompt in response:
                        response = response.replace(prompt, "").strip()
                    
                    return response
                    
                except Exception as e:
                    st.warning(f"æ¨™æº–çš„ãªè§£ææ–¹æ³•ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ä»£æ›¿æ–¹æ³•ã‚’è©¦ã¿ã¾ã™: {e}")
                    
                    # ä»£æ›¿æ–¹æ³•ã‚’è©¦ã™
                    try:
                        # ç”»åƒã®ã¿ã®å…¥åŠ›
                        inputs = processor(images=image, return_tensors="pt").to(model.device)
                        image_embeddings = model.get_image_features(**inputs)
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®æº–å‚™
                        text_inputs = processor(text=prompt, return_tensors="pt").to(model.device)
                        
                        # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å‡¦ç†
                        outputs = model(
                            input_ids=text_inputs.input_ids,
                            attention_mask=text_inputs.attention_mask,
                            pixel_values=inputs.pixel_values if hasattr(inputs, 'pixel_values') else None,
                            image_embeddings=image_embeddings
                        )
                        
                        return "ç”»åƒã‚’åˆ†æã—ã¾ã—ãŸã€‚è©³ç´°æƒ…å ±ã¯å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸãŒã€åŸºæœ¬çš„ãªèªè­˜ã‚’è¡Œã„ã¾ã—ãŸã€‚"
                    except Exception as e2:
                        return f"ç”»åƒèªè­˜ã«è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã¿ã¾ã—ãŸãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {str(e2)}"
        
    except Exception as e:
        return f"ç”»åƒåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°
def generate_text(model, processor, prompt, temperature=0.7, max_tokens=512):
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã†é–¢æ•°"""
    try:
        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):  # éæ¨å¥¨è­¦å‘Šã‚’ä¿®æ­£
            inputs = processor(prompt, return_tensors="pt")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
            for k, v in inputs.items():
                if hasattr(v, "dtype") and v.dtype == torch.float32:
                    inputs[k] = v.to(torch.bfloat16)
            inputs = {k: inputs[k].to(model.device) for k in inputs if k != "token_type_ids"}
            
            # ç”Ÿæˆ
            output = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temperature,
                repetition_penalty=1.2,
                use_cache=True
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
            generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®å‰Šé™¤
            if prompt in generated_text:
                generated_text = generated_text.replace(prompt, "").strip()
            
            return generated_text
    except Exception as e:
        return f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆé–¢æ•° - TextIteratorStreamerã‚’ä½¿ç”¨ã—ãŸå®Ÿè£…ã®è¿½åŠ 
def generate_text_streaming(model, processor, prompt, temperature=0.7, max_tokens=512):
    try:
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®è¨­å®š
        streamer = TextIteratorStreamer(processor, skip_special_tokens=True)
        
        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):  # ä¿®æ­£ç‰ˆã®autocast
            inputs = processor(prompt, return_tensors="pt")
            inputs = {k: inputs[k].to(model.device) for k in inputs if k != "token_type_ids"}
            
            # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ç”Ÿæˆ
            import threading
            
            generation_kwargs = dict(
                **inputs,
                streamer=streamer,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=0.95,
                repetition_penalty=1.1,
            )
            
            thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’è¿”ã™
            return streamer, thread
            
    except Exception as e:
        st.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, None

# UIã®æ§‹ç¯‰
st.title("ãƒã‚¤ãƒ‘ãƒ¼AIç”»åƒç†è§£ãƒãƒ£ãƒƒãƒˆ")
display_cuda_info()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ - ãƒ¢ãƒ‡ãƒ«è¨­å®š
st.sidebar.title("ãƒ¢ãƒ‡ãƒ«è¨­å®š")

# ãƒ¢ãƒ¼ãƒ‰é¸æŠ
mode_options = {
    "standard": "æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ (ä¸€èˆ¬ä¼šè©±)",
    "analytic": "åˆ†æãƒ¢ãƒ¼ãƒ‰ (è©³ç´°ãªæ¨è«–ãƒ»åˆ†æ)",
    "creative": "å‰µé€ ãƒ¢ãƒ¼ãƒ‰ (ç‰©èªãƒ»å‰µä½œ)"
}
selected_mode = st.sidebar.selectbox(
    "ä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰",
    list(mode_options.keys()),
    format_func=lambda x: mode_options[x],
    index=0
)
st.session_state.current_mode = selected_mode

# ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«é¸æŠ
st.sidebar.subheader("ç”»åƒç†è§£ãƒ¢ãƒ‡ãƒ«")
selected_vision = st.sidebar.selectbox(
    "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
    list(VISION_MODELS.keys()),
    format_func=lambda x: f"{VISION_MODELS[x]['name']} ({VISION_MODELS[x]['description']})"
)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«é¸æŠ
st.sidebar.subheader("ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«")
available_text_models = TEXT_MODELS.get(selected_mode, TEXT_MODELS["standard"])
selected_text = st.sidebar.selectbox(
    "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
    list(available_text_models.keys()),
    format_func=lambda x: f"{available_text_models[x]['name']} ({available_text_models[x]['description']})"
)

# VRAMä½¿ç”¨é‡è¨ˆç®—
vision_vram = VISION_MODELS[selected_vision]["vram"]
text_vram = available_text_models[selected_text]["vram"]
total_vram = vision_vram + text_vram

if total_vram > 16:
    st.sidebar.warning(f"âš ï¸ è­¦å‘Š: åˆè¨ˆVRAMä½¿ç”¨é‡({total_vram:.1f}GB)ãŒ16GBã‚’è¶…ãˆã¦ã„ã¾ã™")
else:
    st.sidebar.success(f"VRAMä½¿ç”¨é‡: {total_vram:.1f}GB/16GB")

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ - åŠ¹ç‡åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³
if st.sidebar.button("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"):
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒãƒˆ (é‡è¤‡ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰é˜²æ­¢)
    st.session_state.model_loading = True
    
    # VRAMã‚’ãƒã‚§ãƒƒã‚¯ã€å¿…è¦ãªå ´åˆã®ã¿è§£æ”¾
    vision_vram = VISION_MODELS[selected_vision]["vram"]
    text_vram = available_text_models[selected_text]["vram"]
    total_needed_vram = vision_vram + text_vram
    
    # 1å›ã ã‘ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ã¨è§£æ”¾ã‚’å®Ÿè¡Œ
    need_unload = False
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
        available_memory = total_memory - reserved_memory
        
        # å¿…è¦ãªãƒ¡ãƒ¢ãƒªãŒåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‚’è¶…ãˆã‚‹å ´åˆ
        if total_needed_vram > available_memory * 0.9:
            need_unload = True
    
    # å¿…è¦ãªå ´åˆã®ã¿ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ (1å›ã ã‘)
    if need_unload:
        st.warning("å¿…è¦ãªVRAMå®¹é‡ãŒå¤§ãã„ãŸã‚ã€æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾ã—ã¾ã™")
        if st.session_state.vision_model is not None:
            del st.session_state.vision_model
            st.session_state.vision_model = None
        if st.session_state.text_model is not None:
            del st.session_state.text_model
            st.session_state.text_model = None
        gc.collect()
        torch.cuda.empty_cache()
    
    # ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (åŒã˜ãƒ¢ãƒ‡ãƒ«ãªã‚‰å†åˆ©ç”¨)
    st.session_state.vision_model, st.session_state.vision_processor, st.session_state.current_vision_model = load_vision_model(selected_vision)
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (åŒã˜ãƒ¢ãƒ‡ãƒ«ãªã‚‰å†åˆ©ç”¨)
    st.session_state.text_model, st.session_state.text_processor, st.session_state.current_text_model = load_text_model(selected_text, selected_mode)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
    st.session_state.model_loading = False
    
    # çµæœè¡¨ç¤º
    if st.session_state.vision_model and st.session_state.text_model:
        st.success(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ: {st.session_state.current_vision_model} + {st.session_state.current_text_model}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®è¡¨ç¤º
        display_memory_status()
    else:
        st.error("ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ")

# ç”Ÿæˆè¨­å®š
st.sidebar.subheader("ç”Ÿæˆè¨­å®š")
temperature = st.sidebar.slider("Temperature", 0.1, 1.5, 0.7)
max_tokens = st.sidebar.slider("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°", 100, 2000, 512)

# ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
chat_container = st.container()

with chat_container:
    if not st.session_state.chat_history:
        st.info("ğŸ‘‹ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¦ãã ã•ã„")
    
    for i, chat in enumerate(st.session_state.chat_history):
        col1, col2 = st.columns([1, 9])
        
        if chat["role"] == "user":
            with col1:
                st.image("https://api.dicebear.com/7.x/identicon/svg?seed=user", width=50)
            with col2:
                st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼**")
                st.markdown(chat["content"])
                # ç”»åƒè¡¨ç¤º
                if "images" in chat and chat["images"]:
                    for img in chat["images"]:
                        st.image(img, width=300)
        else:
            with col1:
                st.image("https://api.dicebear.com/7.x/identicon/svg?seed=assistant", width=50)
            with col2:
                if "analysis" in chat and st.session_state.current_mode == "analytic":
                    st.markdown(f"**ç”»åƒåˆ†æ ({st.session_state.current_vision_model})**")
                    st.markdown(chat["analysis"])
                    st.markdown("---")
                st.markdown(f"**AIå¿œç­” ({st.session_state.current_text_model})**")
                st.markdown(chat["content"])

# å…¥åŠ›ã‚¨ãƒªã‚¢
st.subheader("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›")
user_input = st.text_area("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›", height=100, key="user_input")

# ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_files = st.file_uploader("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è¤‡æ•°å¯)", type=["jpg", "jpeg", "png"], accept_multiple_files=True)
uploaded_images = []

if uploaded_files:
    cols = st.columns(min(4, len(uploaded_files)))
    for i, uploaded_file in enumerate(uploaded_files):
        # ç”»åƒå‡¦ç†
        image = Image.open(uploaded_file)
        processed_image = optimize_image(image)
        
        # ä¸€æ™‚ä¿å­˜
        temp_file = NamedTemporaryFile(delete=False, suffix=".jpg")
        temp_file_path = temp_file.name
        processed_image.save(temp_file_path)
        st.session_state.temp_files.append(temp_file_path)
        uploaded_images.append(temp_file_path)
        
        # ã‚µãƒ ãƒã‚¤ãƒ«è¡¨ç¤º
        with cols[i % 4]:
            st.image(processed_image, width=150, caption=f"ç”»åƒ {i+1}")

# é€ä¿¡ãƒœã‚¿ãƒ³
if st.button("é€ä¿¡", key="send_button"):
    if st.session_state.vision_model is None or st.session_state.text_model is None:
        st.error("å…ˆã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    elif user_input.strip() or uploaded_images:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        user_message = {"role": "user", "content": user_input}
        if uploaded_images:
            user_message["images"] = uploaded_images
        st.session_state.chat_history.append(user_message)
        
        # AIå¿œç­”ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        ai_message = {"role": "assistant", "content": ""}
        st.session_state.chat_history.append(ai_message)
        
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨çŠ¶æ…‹è¡¨ç¤ºç”¨
        response_placeholder = st.empty()
        status_placeholder = st.empty()
        
        # ç”»åƒåˆ†æ
        image_descriptions = []
        if uploaded_images:
            with status_placeholder.status("ç”»åƒã‚’åˆ†æä¸­...") as status:
                for i, img_path in enumerate(uploaded_images):
                    status.update(label=f"ç”»åƒ {i+1}/{len(uploaded_images)} ã‚’åˆ†æä¸­...")
                    
                    # ç”»åƒåˆ†æã®å®Ÿè¡Œ
                    image_desc = analyze_image(
                        st.session_state.vision_model,
                        st.session_state.vision_processor,
                        img_path,
                        mode=st.session_state.current_mode
                    )
                    
                    image_descriptions.append(image_desc)
                    
                    # åˆ†æãƒ¢ãƒ¼ãƒ‰ã§ã¯åˆ†æçµæœã‚‚è¡¨ç¤º
                    if st.session_state.current_mode == "analytic":
                        if "analysis" not in ai_message:
                            ai_message["analysis"] = "**ç”»åƒåˆ†æçµæœ:**\n\n"
                        ai_message["analysis"] += f"**ç”»åƒ {i+1}:** {image_desc}\n\n"
                
                status.update(label="ç”»åƒåˆ†æå®Œäº†", state="complete")
        
        # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        if st.session_state.current_mode == "standard":
            if image_descriptions:
                prompt = f"{user_input}\n\nç”»åƒå†…å®¹: {' '.join(image_descriptions)}"
            else:
                prompt = user_input
        elif st.session_state.current_mode == "analytic":
            if image_descriptions:
                prompt = f"ä»¥ä¸‹ã®ç”»åƒåˆ†æã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«è©³ç´°ã‹ã¤è«–ç†çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nç”»åƒåˆ†æçµæœ:\n{' '.join(image_descriptions)}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_input}"
            else:
                prompt = f"ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€è©³ç´°ã‹ã¤è«–ç†çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_input}"
        else:  # creative
            if image_descriptions:
                prompt = f"ä»¥ä¸‹ã®ç”»åƒå†…å®¹ã«åŸºã¥ã„ã¦ã€å‰µé€ çš„ã‹ã¤é­…åŠ›çš„ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\nç”»åƒå†…å®¹:\n{' '.join(image_descriptions)}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}"
            else:
                prompt = f"ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€å‰µé€ çš„ã‹ã¤é­…åŠ›çš„ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\nãƒ†ãƒ¼ãƒ: {user_input}"
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºï¼‰
        with status_placeholder.status("å›ç­”ã‚’ç”Ÿæˆä¸­...") as status:
            try:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚’è¨­å®š
                streamer, thread = generate_text_streaming(
                    st.session_state.text_model,
                    st.session_state.text_processor,
                    prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                if streamer and thread:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
                    full_response = ""
                    
                    with response_placeholder.container():
                        col1, col2 = st.columns([1, 9])
                        with col1:
                            st.image("https://api.dicebear.com/7.x/identicon/svg?seed=assistant", width=50)
                        with col2:
                            if "analysis" in ai_message and st.session_state.current_mode == "analytic":
                                st.markdown(f"**ç”»åƒåˆ†æ ({st.session_state.current_vision_model})**")
                                st.markdown(ai_message["analysis"])
                                st.markdown("---")
                            
                            # å¿œç­”è¡¨ç¤ºç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                            response_text = st.empty()
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å—ã‘å–ã‚‹
                    for new_text in streamer:
                        full_response += new_text
                        # UIã‚’æ›´æ–°
                        response_text.markdown(f"**AIå¿œç­” ({st.session_state.current_text_model})**\n\n{full_response}â–Œ")
                        status.update(label=f"å›ç­”ã‚’ç”Ÿæˆä¸­... ({len(full_response)}æ–‡å­—)")
                    
                    # ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…ã¤
                    thread.join()
                    
                    # æœ€çµ‚çš„ãªå¿œç­”ã‚’è¨­å®š
                    ai_message["content"] = full_response
                    status.update(label="å›ç­”ç”Ÿæˆå®Œäº†", state="complete")
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ç”Ÿæˆ
                    final_response = generate_text(
                        st.session_state.text_model,
                        st.session_state.text_processor,
                        prompt,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    # UIã‚’æ›´æ–°
                    with response_placeholder.container():
                        col1, col2 = st.columns([1, 9])
                        with col1:
                            st.image("https://api.dicebear.com/7.x/identicon/svg?seed=assistant", width=50)
                        with col2:
                            if "analysis" in ai_message and st.session_state.current_mode == "analytic":
                                st.markdown(f"**ç”»åƒåˆ†æ ({st.session_state.current_vision_model})**")
                                st.markdown(ai_message["analysis"])
                                st.markdown("---")
                            st.markdown(f"**AIå¿œç­” ({st.session_state.current_text_model})**")
                            st.markdown(final_response)
                    
                    ai_message["content"] = final_response
                    status.update(label="å›ç­”ç”Ÿæˆå®Œäº†", state="complete")
                    
            except Exception as gen_error:
                st.error(f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {gen_error}")
                ai_message["content"] = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(gen_error)}"
        
        # ç”»é¢ã‚’æ›´æ–°
        st.rerun()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ã‚¯ãƒªã‚¢
if st.button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.chat_history = []
    st.rerun()  # experimental_rerunã‹ã‚‰å¤‰æ›´

# ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤º
with st.sidebar.expander("ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³", expanded=False):
    display_memory_status()
    
    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.button("ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾"):
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ã§ãªã„å ´åˆã®ã¿å®Ÿè¡Œ
        if not hasattr(st.session_state, "model_loading") or not st.session_state.model_loading:
            if st.session_state.vision_model is not None:
                del st.session_state.vision_model
                st.session_state.vision_model = None
            if st.session_state.text_model is not None:
                del st.session_state.text_model
                st.session_state.text_model = None
            gc.collect()
            torch.cuda.empty_cache()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            st.session_state.current_vision_model = None
            st.session_state.current_text_model = None
            st.success("ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã‹ã‚‰è§£æ”¾ã—ã¾ã—ãŸ")
            st.rerun()
